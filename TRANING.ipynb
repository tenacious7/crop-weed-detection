{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3bcb3-419a-4124-80f8-0cce3ef609aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "SYNTHETIC_DATA_DIR = r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\images\"\n",
    "MASK_DIR = r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\masks\"\n",
    "VISUALIZATION_DIR = r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\visualization\"\n",
    "\n",
    "class LightweightUNet(nn.Module):\n",
    "    def __init__(self, n_classes=3):\n",
    "        super(LightweightUNet, self).__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = self._conv_block(3, 32)\n",
    "        self.enc2 = self._conv_block(32, 64)\n",
    "        self.enc3 = self._conv_block(64, 128)\n",
    "        self.enc4 = self._conv_block(128, 256)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = self._conv_block(256, 128)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = self._conv_block(128, 64)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = self._conv_block(64, 32)\n",
    "        \n",
    "        self.final = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def _conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.max_pool(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.max_pool(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.max_pool(e3)\n",
    "        \n",
    "        # Bridge\n",
    "        e4 = self.enc4(p3)\n",
    "        \n",
    "        # Decoder path\n",
    "        d3 = self.up3(e4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        return self.final(d1)\n",
    "\n",
    "class PlantSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = [img for img in sorted(os.listdir(image_dir)) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name.replace('synthetic', 'mask'))\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Image not found or unable to read at {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load mask\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                raise ValueError(f\"Mask not found or unable to read at {mask_path}\")\n",
    "            \n",
    "            # One-hot encode mask\n",
    "            mask_one_hot = np.zeros((3, mask.shape[0], mask.shape[1]), dtype=np.float32)\n",
    "            for i in range(3):\n",
    "                mask_one_hot[i, :, :] = (mask == i).astype(np.float32)\n",
    "\n",
    "            # Apply transformations\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, mask=mask_one_hot.transpose(1, 2, 0))\n",
    "                image = augmented['image']\n",
    "                mask_one_hot = augmented['mask'].permute(2, 0, 1)  # Change from HWC to CHW format\n",
    "\n",
    "            return image, mask_one_hot\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {img_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "def get_transforms(train=True):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.GaussNoise(p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "def plot_training_history(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with tqdm(dataloader, desc='Training') as pbar:\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def main():\n",
    "    # Set up paths and parameters\n",
    "    BATCH_SIZE = 4  # Reduced batch size\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Datasets and loaders\n",
    "    train_dataset = PlantSegmentationDataset(\n",
    "        SYNTHETIC_DATA_DIR,\n",
    "        MASK_DIR,\n",
    "        transform=get_transforms(train=True)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # No multiprocessing\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model, criterion, optimizer\n",
    "    model = LightweightUNet().to(DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Plot and save training history\n",
    "    plot_training_history(train_losses, [])\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc615d51-f179-4306-9d91-97fe0a7d903b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_iou, mean_dice, mean_pixel_accuracy\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Evaluate on training set (or create a validation set if available)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m train_iou, train_dice, train_pixel_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, train_loader, DEVICE)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Dice Coefficient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define functions for metrics\n",
    "def compute_iou(pred, mask, threshold=0.5):\n",
    "    pred = (pred > threshold).float()  # Convert to binary\n",
    "    intersection = (pred * mask).sum()\n",
    "    union = pred.sum() + mask.sum() - intersection\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union\n",
    "\n",
    "def compute_dice(pred, mask, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * mask).sum()\n",
    "    dice = (2.0 * intersection) / (pred.sum() + mask.sum())\n",
    "    return dice\n",
    "\n",
    "def compute_pixel_accuracy(pred, mask, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    correct_pixels = (pred == mask).sum().float()\n",
    "    total_pixels = pred.numel()\n",
    "    return correct_pixels / total_pixels\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "    pixel_accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            outputs = torch.sigmoid(outputs)  # Apply sigmoid for probability outputs\n",
    "            \n",
    "            # Compute metrics for each image in the batch\n",
    "            for i in range(len(images)):\n",
    "                pred = outputs[i]\n",
    "                mask = masks[i]\n",
    "                \n",
    "                iou = compute_iou(pred, mask)\n",
    "                dice = compute_dice(pred, mask)\n",
    "                pixel_acc = compute_pixel_accuracy(pred, mask)\n",
    "                \n",
    "                iou_scores.append(iou.item())\n",
    "                dice_scores.append(dice.item())\n",
    "                pixel_accuracies.append(pixel_acc.item())\n",
    "                \n",
    "    # Mean metrics\n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    mean_pixel_accuracy = np.mean(pixel_accuracies)\n",
    "    \n",
    "    return mean_iou, mean_dice, mean_pixel_accuracy\n",
    "\n",
    "# Evaluate on training set (or create a validation set if available)\n",
    "train_iou, train_dice, train_pixel_accuracy = evaluate_model(model, train_loader, DEVICE)\n",
    "\n",
    "print(f\"Training IoU: {train_iou:.4f}\")\n",
    "print(f\"Training Dice Coefficient: {train_dice:.4f}\")\n",
    "print(f\"Training Pixel Accuracy: {train_pixel_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fb872-6011-43c6-8bef-a82663624f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
