{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2383d7ab-22fc-4ea3-a0c6-3dbd9d8aebb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ANACONDA3\\envs\\gpu\\lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "2024-11-02 09:50:38,863 - INFO - Using device: cuda\n",
      "2024-11-02 09:50:39,277 - INFO - \n",
      "Epoch 1/50\n",
      "Training:   0%|                                                                                | 0/250 [00:05<?, ?it/s]\n",
      "2024-11-02 09:50:44,847 - ERROR - Training failed with error: DataLoader worker (pid(s) 24700, 9744, 8316, 9100) exited unexpectedly\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 24700, 9744, 8316, 9100) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ANACONDA3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mC:\\ANACONDA3\\envs\\gpu\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 312\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 312\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 265\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_EPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    268\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 191\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m    188\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m    192\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    193\u001b[0m         masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ANACONDA3\\envs\\gpu\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ANACONDA3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\ANACONDA3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ANACONDA3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mC:\\ANACONDA3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 24700, 9744, 8316, 9100) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Paths\n",
    "SYNTHETIC_DATA_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\images\")\n",
    "MASK_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\masks\")\n",
    "VISUALIZATION_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\visualization\")\n",
    "MODELS_DIR = Path(\"saved_models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class LightweightUNet(nn.Module):\n",
    "    def __init__(self, n_classes=3):\n",
    "        super(LightweightUNet, self).__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = self._conv_block(3, 32)\n",
    "        self.enc2 = self._conv_block(32, 64)\n",
    "        self.enc3 = self._conv_block(64, 128)\n",
    "        self.enc4 = self._conv_block(128, 256)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = self._conv_block(256, 128)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = self._conv_block(128, 64)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = self._conv_block(64, 32)\n",
    "        \n",
    "        self.final = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def _conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder path with skip connections\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.max_pool(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.max_pool(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.max_pool(e3)\n",
    "        \n",
    "        # Bridge\n",
    "        e4 = self.enc4(p3)\n",
    "        \n",
    "        # Decoder path\n",
    "        d3 = self.up3(e4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        return self.final(d1)\n",
    "\n",
    "class PlantSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.transform = transform\n",
    "        self.images = [img for img in sorted(os.listdir(image_dir)) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "        mask_path = self.mask_dir / img_name.replace('synthetic', 'mask')\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = cv2.imread(str(img_path))\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Image not found or unable to read at {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load mask\n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                raise ValueError(f\"Mask not found or unable to read at {mask_path}\")\n",
    "            \n",
    "            # One-hot encode mask\n",
    "            mask_one_hot = np.zeros((3, mask.shape[0], mask.shape[1]), dtype=np.float32)\n",
    "            for i in range(3):\n",
    "                mask_one_hot[i, :, :] = (mask == i).astype(np.float32)\n",
    "\n",
    "            # Apply transformations\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, mask=mask_one_hot.transpose(1, 2, 0))\n",
    "                image = augmented['image']\n",
    "                mask_one_hot = augmented['mask'].permute(2, 0, 1)\n",
    "\n",
    "            return image, mask_one_hot\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading file {img_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "def get_transforms(train=True):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.GaussNoise(p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "def save_model(model, optimizer, epoch, train_loss, save_dir, filename):\n",
    "    \"\"\"Save model checkpoint with additional training information\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "    }\n",
    "    \n",
    "    save_path = save_dir / filename\n",
    "    torch.save(checkpoint, save_path)\n",
    "    logging.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['train_loss']\n",
    "\n",
    "def plot_training_history(train_losses, save_dir):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    \n",
    "    save_path = save_dir / f'training_history_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Training history plot saved to {save_path}\")\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with tqdm(dataloader, desc='Training') as pbar:\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def main():\n",
    "    # Training configuration\n",
    "    config = {\n",
    "        'BATCH_SIZE': 4,\n",
    "        'NUM_EPOCHS': 50,\n",
    "        'LEARNING_RATE': 0.001,\n",
    "        'SAVE_FREQUENCY': 5,  # Save model every N epochs\n",
    "        'NUM_WORKERS': 4 if torch.cuda.is_available() else 0,  # Use multiple workers if GPU available\n",
    "    }\n",
    "    \n",
    "    # Set up device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize CUDA performance\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = PlantSegmentationDataset(\n",
    "        SYNTHETIC_DATA_DIR,\n",
    "        MASK_DIR,\n",
    "        transform=get_transforms(train=True)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['BATCH_SIZE'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['NUM_WORKERS'],\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True if config['NUM_WORKERS'] > 0 else False\n",
    "    )\n",
    "    \n",
    "    # Initialize model, criterion, optimizer\n",
    "    model = LightweightUNet().to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        logging.info(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])\n",
    "    \n",
    "    # Optional: Load checkpoint if exists\n",
    "    checkpoint_path = MODELS_DIR / 'latest_checkpoint.pth'\n",
    "    start_epoch = 0\n",
    "    train_losses = []\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        start_epoch, prev_losses = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "        train_losses = prev_losses\n",
    "        logging.info(f\"Resumed training from epoch {start_epoch}\")\n",
    "    \n",
    "    # Training loop\n",
    "    try:\n",
    "        for epoch in range(start_epoch, config['NUM_EPOCHS']):\n",
    "            logging.info(f\"\\nEpoch {epoch+1}/{config['NUM_EPOCHS']}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            logging.info(f\"Train Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (epoch + 1) % config['SAVE_FREQUENCY'] == 0:\n",
    "                save_model(\n",
    "                    model if not isinstance(model, nn.DataParallel) else model.module,\n",
    "                    optimizer,\n",
    "                    epoch + 1,\n",
    "                    train_losses,\n",
    "                    MODELS_DIR,\n",
    "                    f'checkpoint_epoch_{epoch+1}.pth'\n",
    "                )\n",
    "        \n",
    "        # Save final model\n",
    "        save_model(\n",
    "            model if not isinstance(model, nn.DataParallel) else model.module,\n",
    "            optimizer,\n",
    "            config['NUM_EPOCHS'],\n",
    "            train_losses,\n",
    "            MODELS_DIR,\n",
    "            'final_model.pth'\n",
    "        )\n",
    "        \n",
    "        # Plot and save training history\n",
    "        plot_training_history(train_losses, MODELS_DIR)\n",
    "        logging.info(\"Training completed successfully!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Training interrupted by user\")\n",
    "        # Save checkpoint on interruption\n",
    "        save_model(\n",
    "            model if not isinstance(model, nn.DataParallel) else model.module,\n",
    "            optimizer,\n",
    "            epoch + 1,\n",
    "            train_losses,\n",
    "            MODELS_DIR,\n",
    "            'interrupted_checkpoint.pth'\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed with error: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dfeed93-fd83-4123-8d08-5aca182ec9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 09:55:16,738 - INFO - Using device: cuda\n",
      "2024-11-02 09:55:17,009 - ERROR - Training failed: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing, otherwise set prefetch_factor to None.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Paths\n",
    "SYNTHETIC_DATA_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\images\")\n",
    "MASK_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\masks\")\n",
    "VISUALIZATION_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\visualization\")\n",
    "MODELS_DIR = Path(\"saved_models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class LightweightUNet(nn.Module):\n",
    "    def __init__(self, n_classes=3):\n",
    "        super(LightweightUNet, self).__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = self._conv_block(3, 32)\n",
    "        self.enc2 = self._conv_block(32, 64)\n",
    "        self.enc3 = self._conv_block(64, 128)\n",
    "        self.enc4 = self._conv_block(128, 256)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = self._conv_block(256, 128)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = self._conv_block(128, 64)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = self._conv_block(64, 32)\n",
    "        \n",
    "        self.final = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def _conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder path with skip connections\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.max_pool(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.max_pool(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.max_pool(e3)\n",
    "        \n",
    "        # Bridge\n",
    "        e4 = self.enc4(p3)\n",
    "        \n",
    "        # Decoder path\n",
    "        d3 = self.up3(e4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        return self.final(d1)\n",
    "\n",
    "class PlantSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Validate directories exist\n",
    "        if not self.image_dir.exists():\n",
    "            raise ValueError(f\"Image directory not found: {image_dir}\")\n",
    "        if not self.mask_dir.exists():\n",
    "            raise ValueError(f\"Mask directory not found: {mask_dir}\")\n",
    "            \n",
    "        # Get valid image files\n",
    "        self.images = []\n",
    "        for img in sorted(os.listdir(image_dir)):\n",
    "            if img.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                # Check if corresponding mask exists\n",
    "                mask_name = img.replace('synthetic', 'mask')\n",
    "                if (self.mask_dir / mask_name).exists():\n",
    "                    self.images.append(img)\n",
    "                else:\n",
    "                    logging.warning(f\"Skipping {img} - no corresponding mask found\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.images):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset of size {len(self.images)}\")\n",
    "            \n",
    "        img_name = self.images[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "        mask_path = self.mask_dir / img_name.replace('synthetic', 'mask')\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = cv2.imread(str(img_path))\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to read image at {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load mask\n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                raise ValueError(f\"Failed to read mask at {mask_path}\")\n",
    "            \n",
    "            # Validate image and mask dimensions match\n",
    "            if image.shape[:2] != mask.shape[:2]:\n",
    "                raise ValueError(f\"Image and mask dimensions don't match for {img_name}\")\n",
    "            \n",
    "            # One-hot encode mask\n",
    "            mask_one_hot = np.zeros((3, mask.shape[0], mask.shape[1]), dtype=np.float32)\n",
    "            for i in range(3):\n",
    "                mask_one_hot[i, :, :] = (mask == i).astype(np.float32)\n",
    "\n",
    "            # Apply transformations\n",
    "            if self.transform:\n",
    "                try:\n",
    "                    augmented = self.transform(image=image, mask=mask_one_hot.transpose(1, 2, 0))\n",
    "                    image = augmented['image']\n",
    "                    mask_one_hot = augmented['mask'].permute(2, 0, 1)\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Transform failed for {img_name}: {str(e)}\")\n",
    "\n",
    "            return image, mask_one_hot\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {img_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def get_transforms(train=True):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.GaussNoise(p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "def save_model(model, optimizer, epoch, train_loss, save_dir, filename):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "    }\n",
    "    \n",
    "    save_path = save_dir / filename\n",
    "    torch.save(checkpoint, save_path)\n",
    "    logging.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with tqdm(dataloader, desc='Training') as pbar:\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def main():\n",
    "    # Training configuration\n",
    "    config = {\n",
    "        'BATCH_SIZE': 4,\n",
    "        'NUM_EPOCHS': 50,\n",
    "        'LEARNING_RATE': 0.001,\n",
    "        'SAVE_FREQUENCY': 5,\n",
    "        'NUM_WORKERS': 0,\n",
    "    }\n",
    "    \n",
    "    # Set up device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    try:\n",
    "        train_dataset = PlantSegmentationDataset(\n",
    "            SYNTHETIC_DATA_DIR,\n",
    "            MASK_DIR,\n",
    "            transform=get_transforms(train=True)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config['BATCH_SIZE'],\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "            persistent_workers=False,\n",
    "            prefetch_factor=2,\n",
    "            timeout=60,\n",
    "        )\n",
    "        \n",
    "        model = LightweightUNet(n_classes=3).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(config['NUM_EPOCHS']):\n",
    "            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            logging.info(f\"Epoch [{epoch + 1}/{config['NUM_EPOCHS']}], Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            if (epoch + 1) % config['SAVE_FREQUENCY'] == 0:\n",
    "                save_model(model, optimizer, epoch + 1, train_loss, MODELS_DIR, f\"model_epoch_{epoch + 1}.pth\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb8f12-55b0-459a-81aa-80cf8f2f39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 10:00:32,369 - INFO - Using device: cuda\n",
      "2024-11-02 10:00:32,524 - INFO - Epoch 1/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:58<00:00,  4.27it/s, loss=0.1432]\n",
      "2024-11-02 10:01:31,137 - INFO - Epoch 1/50 - Loss: 0.2285\n",
      "2024-11-02 10:01:31,137 - INFO - Epoch 2/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.14it/s, loss=0.0795]\n",
      "2024-11-02 10:02:19,754 - INFO - Epoch 2/50 - Loss: 0.1227\n",
      "2024-11-02 10:02:19,755 - INFO - Epoch 3/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.10it/s, loss=0.1581]\n",
      "2024-11-02 10:03:08,755 - INFO - Epoch 3/50 - Loss: 0.1094\n",
      "2024-11-02 10:03:08,755 - INFO - Epoch 4/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.15it/s, loss=0.0697]\n",
      "2024-11-02 10:03:57,273 - INFO - Epoch 4/50 - Loss: 0.1061\n",
      "2024-11-02 10:03:57,274 - INFO - Epoch 5/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.15it/s, loss=0.0845]\n",
      "2024-11-02 10:04:45,872 - INFO - Epoch 5/50 - Loss: 0.1005\n",
      "2024-11-02 10:04:45,944 - INFO - Model saved to saved_models\\model_epoch_5.pt\n",
      "2024-11-02 10:04:45,944 - INFO - Epoch 6/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.09it/s, loss=0.0741]\n",
      "2024-11-02 10:05:35,067 - INFO - Epoch 6/50 - Loss: 0.0985\n",
      "2024-11-02 10:05:35,067 - INFO - Epoch 7/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.10it/s, loss=0.1208]\n",
      "2024-11-02 10:06:24,077 - INFO - Epoch 7/50 - Loss: 0.0958\n",
      "2024-11-02 10:06:24,077 - INFO - Epoch 8/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.12it/s, loss=0.1433]\n",
      "2024-11-02 10:07:12,897 - INFO - Epoch 8/50 - Loss: 0.0946\n",
      "2024-11-02 10:07:12,899 - INFO - Epoch 9/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.15it/s, loss=0.0892]\n",
      "2024-11-02 10:08:01,442 - INFO - Epoch 9/50 - Loss: 0.0979\n",
      "2024-11-02 10:08:01,445 - INFO - Epoch 10/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.11it/s, loss=0.0850]\n",
      "2024-11-02 10:08:50,371 - INFO - Epoch 10/50 - Loss: 0.0929\n",
      "2024-11-02 10:08:50,450 - INFO - Model saved to saved_models\\model_epoch_10.pt\n",
      "2024-11-02 10:08:50,452 - INFO - Epoch 11/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.14it/s, loss=0.1487]\n",
      "2024-11-02 10:09:39,064 - INFO - Epoch 11/50 - Loss: 0.0908\n",
      "2024-11-02 10:09:39,064 - INFO - Epoch 12/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.08it/s, loss=0.1156]\n",
      "2024-11-02 10:10:28,310 - INFO - Epoch 12/50 - Loss: 0.0889\n",
      "2024-11-02 10:10:28,312 - INFO - Epoch 13/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.07it/s, loss=0.0735]\n",
      "2024-11-02 10:11:17,603 - INFO - Epoch 13/50 - Loss: 0.0885\n",
      "2024-11-02 10:11:17,609 - INFO - Epoch 14/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.11it/s, loss=0.0947]\n",
      "2024-11-02 10:12:06,512 - INFO - Epoch 14/50 - Loss: 0.0875\n",
      "2024-11-02 10:12:06,514 - INFO - Epoch 15/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.13it/s, loss=0.1090]\n",
      "2024-11-02 10:12:55,266 - INFO - Epoch 15/50 - Loss: 0.0869\n",
      "2024-11-02 10:12:55,342 - INFO - Model saved to saved_models\\model_epoch_15.pt\n",
      "2024-11-02 10:12:55,345 - INFO - Epoch 16/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.09it/s, loss=0.0721]\n",
      "2024-11-02 10:13:44,462 - INFO - Epoch 16/50 - Loss: 0.0860\n",
      "2024-11-02 10:13:44,462 - INFO - Epoch 17/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.14it/s, loss=0.0929]\n",
      "2024-11-02 10:14:33,123 - INFO - Epoch 17/50 - Loss: 0.0869\n",
      "2024-11-02 10:14:33,123 - INFO - Epoch 18/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.16it/s, loss=0.0945]\n",
      "2024-11-02 10:15:21,589 - INFO - Epoch 18/50 - Loss: 0.0851\n",
      "2024-11-02 10:15:21,590 - INFO - Epoch 19/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [06:06<00:00,  1.47s/it, loss=0.0786]\n",
      "2024-11-02 10:21:28,482 - INFO - Epoch 19/50 - Loss: 0.0832\n",
      "2024-11-02 10:21:28,482 - INFO - Epoch 20/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.03it/s, loss=0.0593]\n",
      "2024-11-02 10:22:18,168 - INFO - Epoch 20/50 - Loss: 0.0825\n",
      "2024-11-02 10:22:18,244 - INFO - Model saved to saved_models\\model_epoch_20.pt\n",
      "2024-11-02 10:22:18,252 - INFO - Epoch 21/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.01it/s, loss=0.0765]\n",
      "2024-11-02 10:23:08,194 - INFO - Epoch 21/50 - Loss: 0.0818\n",
      "2024-11-02 10:23:08,198 - INFO - Epoch 22/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.05it/s, loss=0.1013]\n",
      "2024-11-02 10:23:57,695 - INFO - Epoch 22/50 - Loss: 0.0803\n",
      "2024-11-02 10:23:57,698 - INFO - Epoch 23/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.02it/s, loss=0.0764]\n",
      "2024-11-02 10:24:47,498 - INFO - Epoch 23/50 - Loss: 0.0797\n",
      "2024-11-02 10:24:47,498 - INFO - Epoch 24/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:50<00:00,  4.99it/s, loss=0.1167]\n",
      "2024-11-02 10:25:37,578 - INFO - Epoch 24/50 - Loss: 0.0796\n",
      "2024-11-02 10:25:37,578 - INFO - Epoch 25/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.03it/s, loss=0.0787]\n",
      "2024-11-02 10:26:27,294 - INFO - Epoch 25/50 - Loss: 0.0792\n",
      "2024-11-02 10:26:27,370 - INFO - Model saved to saved_models\\model_epoch_25.pt\n",
      "2024-11-02 10:26:27,370 - INFO - Epoch 26/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.00it/s, loss=0.0623]\n",
      "2024-11-02 10:27:17,342 - INFO - Epoch 26/50 - Loss: 0.0781\n",
      "2024-11-02 10:27:17,344 - INFO - Epoch 27/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.00it/s, loss=0.0781]\n",
      "2024-11-02 10:28:07,334 - INFO - Epoch 27/50 - Loss: 0.0803\n",
      "2024-11-02 10:28:07,337 - INFO - Epoch 28/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.03it/s, loss=0.0744]\n",
      "2024-11-02 10:28:57,054 - INFO - Epoch 28/50 - Loss: 0.0831\n",
      "2024-11-02 10:28:57,054 - INFO - Epoch 29/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.01it/s, loss=0.0833]\n",
      "2024-11-02 10:29:46,953 - INFO - Epoch 29/50 - Loss: 0.0765\n",
      "2024-11-02 10:29:46,955 - INFO - Epoch 30/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.02it/s, loss=0.0580]\n",
      "2024-11-02 10:30:36,777 - INFO - Epoch 30/50 - Loss: 0.0762\n",
      "2024-11-02 10:30:36,848 - INFO - Model saved to saved_models\\model_epoch_30.pt\n",
      "2024-11-02 10:30:36,850 - INFO - Epoch 31/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:56<00:00,  4.40it/s, loss=0.0725]\n",
      "2024-11-02 10:31:33,708 - INFO - Epoch 31/50 - Loss: 0.0748\n",
      "2024-11-02 10:31:33,708 - INFO - Epoch 32/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.01it/s, loss=0.0972]\n",
      "2024-11-02 10:32:23,600 - INFO - Epoch 32/50 - Loss: 0.0743\n",
      "2024-11-02 10:32:23,609 - INFO - Epoch 33/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.07it/s, loss=0.0880]\n",
      "2024-11-02 10:33:12,901 - INFO - Epoch 33/50 - Loss: 0.0736\n",
      "2024-11-02 10:33:12,902 - INFO - Epoch 34/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.06it/s, loss=0.0912]\n",
      "2024-11-02 10:34:02,325 - INFO - Epoch 34/50 - Loss: 0.0739\n",
      "2024-11-02 10:34:02,325 - INFO - Epoch 35/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.07it/s, loss=0.0616]\n",
      "2024-11-02 10:34:51,670 - INFO - Epoch 35/50 - Loss: 0.0729\n",
      "2024-11-02 10:34:51,744 - INFO - Model saved to saved_models\\model_epoch_35.pt\n",
      "2024-11-02 10:34:51,749 - INFO - Epoch 36/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.03it/s, loss=0.0387]\n",
      "2024-11-02 10:35:41,408 - INFO - Epoch 36/50 - Loss: 0.0725\n",
      "2024-11-02 10:35:41,408 - INFO - Epoch 37/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:50<00:00,  4.95it/s, loss=0.0735]\n",
      "2024-11-02 10:36:31,912 - INFO - Epoch 37/50 - Loss: 0.0725\n",
      "2024-11-02 10:36:31,912 - INFO - Epoch 38/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:54<00:00,  4.60it/s, loss=0.0437]\n",
      "2024-11-02 10:37:26,332 - INFO - Epoch 38/50 - Loss: 0.0718\n",
      "2024-11-02 10:37:26,332 - INFO - Epoch 39/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:55<00:00,  4.54it/s, loss=0.0751]\n",
      "2024-11-02 10:38:21,377 - INFO - Epoch 39/50 - Loss: 0.0713\n",
      "2024-11-02 10:38:21,378 - INFO - Epoch 40/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:55<00:00,  4.52it/s, loss=0.0784]\n",
      "2024-11-02 10:39:16,641 - INFO - Epoch 40/50 - Loss: 0.0711\n",
      "2024-11-02 10:39:16,746 - INFO - Model saved to saved_models\\model_epoch_40.pt\n",
      "2024-11-02 10:39:16,749 - INFO - Epoch 41/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:54<00:00,  4.56it/s, loss=0.1023]\n",
      "2024-11-02 10:40:11,568 - INFO - Epoch 41/50 - Loss: 0.0703\n",
      "2024-11-02 10:40:11,568 - INFO - Epoch 42/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:54<00:00,  4.58it/s, loss=0.0321]\n",
      "2024-11-02 10:41:06,202 - INFO - Epoch 42/50 - Loss: 0.0712\n",
      "2024-11-02 10:41:06,204 - INFO - Epoch 43/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:55<00:00,  4.54it/s, loss=0.0598]\n",
      "2024-11-02 10:42:01,250 - INFO - Epoch 43/50 - Loss: 0.0699\n",
      "2024-11-02 10:42:01,250 - INFO - Epoch 44/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:55<00:00,  4.52it/s, loss=0.0858]\n",
      "2024-11-02 10:42:56,564 - INFO - Epoch 44/50 - Loss: 0.0694\n",
      "2024-11-02 10:42:56,564 - INFO - Epoch 45/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:55<00:00,  4.53it/s, loss=0.0540]\n",
      "2024-11-02 10:43:51,723 - INFO - Epoch 45/50 - Loss: 0.0684\n",
      "2024-11-02 10:43:51,849 - INFO - Model saved to saved_models\\model_epoch_45.pt\n",
      "2024-11-02 10:43:51,851 - INFO - Epoch 46/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:55<00:00,  4.54it/s, loss=0.0579]\n",
      "2024-11-02 10:44:46,923 - INFO - Epoch 46/50 - Loss: 0.0684\n",
      "2024-11-02 10:44:46,931 - INFO - Epoch 47/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:54<00:00,  4.55it/s, loss=0.0425]\n",
      "2024-11-02 10:45:41,949 - INFO - Epoch 47/50 - Loss: 0.0680\n",
      "2024-11-02 10:45:41,949 - INFO - Epoch 48/50\n",
      "Training: 100%|█████████████████████████████████████████████████████████| 250/250 [00:55<00:00,  4.53it/s, loss=0.0576]\n",
      "2024-11-02 10:46:37,124 - INFO - Epoch 48/50 - Loss: 0.0678\n",
      "2024-11-02 10:46:37,124 - INFO - Epoch 49/50\n",
      "Training:  30%|█████████████████▏                                        | 74/250 [00:16<00:38,  4.59it/s, loss=0.0396]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Paths\n",
    "SYNTHETIC_DATA_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\images\")\n",
    "MASK_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\masks\")\n",
    "VISUALIZATION_DIR = Path(r\"C:\\IMMAGE ANALYSIS PROJECT DATASET\\weed\\weeds\\output\\visualization\")\n",
    "MODELS_DIR = Path(\"saved_models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class LightweightUNet(nn.Module):\n",
    "    def __init__(self, n_classes=3):\n",
    "        super(LightweightUNet, self).__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = self._conv_block(3, 32)\n",
    "        self.enc2 = self._conv_block(32, 64)\n",
    "        self.enc3 = self._conv_block(64, 128)\n",
    "        self.enc4 = self._conv_block(128, 256)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = self._conv_block(256, 128)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = self._conv_block(128, 64)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = self._conv_block(64, 32)\n",
    "        \n",
    "        self.final = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def _conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder path with skip connections\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.max_pool(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.max_pool(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.max_pool(e3)\n",
    "        \n",
    "        # Bridge\n",
    "        e4 = self.enc4(p3)\n",
    "        \n",
    "        # Decoder path\n",
    "        d3 = self.up3(e4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        return self.final(d1)\n",
    "\n",
    "class PlantSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Validate directories exist\n",
    "        if not self.image_dir.exists():\n",
    "            raise ValueError(f\"Image directory not found: {image_dir}\")\n",
    "        if not self.mask_dir.exists():\n",
    "            raise ValueError(f\"Mask directory not found: {mask_dir}\")\n",
    "            \n",
    "        # Get valid image files\n",
    "        self.images = []\n",
    "        for img in sorted(os.listdir(image_dir)):\n",
    "            if img.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                # Check if corresponding mask exists\n",
    "                mask_name = img.replace('synthetic', 'mask')\n",
    "                if (self.mask_dir / mask_name).exists():\n",
    "                    self.images.append(img)\n",
    "                else:\n",
    "                    logging.warning(f\"Skipping {img} - no corresponding mask found\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.images):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset of size {len(self.images)}\")\n",
    "            \n",
    "        img_name = self.images[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "        mask_path = self.mask_dir / img_name.replace('synthetic', 'mask')\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = cv2.imread(str(img_path))\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to read image at {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load mask\n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                raise ValueError(f\"Failed to read mask at {mask_path}\")\n",
    "            \n",
    "            # Validate image and mask dimensions match\n",
    "            if image.shape[:2] != mask.shape[:2]:\n",
    "                raise ValueError(f\"Image and mask dimensions don't match for {img_name}\")\n",
    "            \n",
    "            # One-hot encode mask\n",
    "            mask_one_hot = np.zeros((3, mask.shape[0], mask.shape[1]), dtype=np.float32)\n",
    "            for i in range(3):\n",
    "                mask_one_hot[i, :, :] = (mask == i).astype(np.float32)\n",
    "\n",
    "            # Apply transformations\n",
    "            if self.transform:\n",
    "                try:\n",
    "                    augmented = self.transform(image=image, mask=mask_one_hot.transpose(1, 2, 0))\n",
    "                    image = augmented['image']\n",
    "                    mask_one_hot = augmented['mask'].permute(2, 0, 1)\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Transform failed for {img_name}: {str(e)}\")\n",
    "\n",
    "            return image, mask_one_hot\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {img_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def get_transforms(train=True):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.GaussNoise(p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "def save_model(model, optimizer, epoch, train_loss, save_dir, filename):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "    }\n",
    "    \n",
    "    save_path = save_dir / filename\n",
    "    torch.save(checkpoint, save_path)\n",
    "    logging.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with tqdm(dataloader, desc='Training') as pbar:\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def main():\n",
    "    # Training configuration\n",
    "    config = {\n",
    "        'BATCH_SIZE': 4,\n",
    "        'NUM_EPOCHS': 50,\n",
    "        'LEARNING_RATE': 0.001,\n",
    "        'SAVE_FREQUENCY': 5,\n",
    "        'NUM_WORKERS': 0,\n",
    "    }\n",
    "    \n",
    "    # Set up device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    try:\n",
    "        train_dataset = PlantSegmentationDataset(\n",
    "            SYNTHETIC_DATA_DIR,\n",
    "            MASK_DIR,\n",
    "            transform=get_transforms(train=True)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config['BATCH_SIZE'],\n",
    "            shuffle=True,\n",
    "            num_workers=config['NUM_WORKERS'],\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=config['NUM_WORKERS'] > 0,\n",
    "            prefetch_factor=(2 if config['NUM_WORKERS'] > 0 else None),\n",
    "            timeout=0,\n",
    "            drop_last=True  # Ensure batches are complete\n",
    "        )\n",
    "        \n",
    "        model = LightweightUNet(n_classes=3).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(config['NUM_EPOCHS']):\n",
    "            logging.info(f\"Epoch {epoch + 1}/{config['NUM_EPOCHS']}\")\n",
    "            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "            logging.info(f\"Epoch {epoch + 1}/{config['NUM_EPOCHS']} - Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            if (epoch + 1) % config['SAVE_FREQUENCY'] == 0:\n",
    "                save_model(model, optimizer, epoch, train_loss, MODELS_DIR, f\"model_epoch_{epoch + 1}.pt\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca04fb-bb7a-4026-9753-8e979bb6c93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu)",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
